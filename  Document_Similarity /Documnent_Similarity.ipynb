{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Text Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:-\n",
    "\n",
    "####  Cleaning Text:\n",
    "* Clealing Text using Regular Expression.\n",
    "#### Tokenizing:\n",
    "* Splitting sentences and words from the body of text. Words are separated by space after the word, i.e.after every word there is a space.\n",
    "#### Stop Words:\n",
    "* Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words . Stop words can be filtered from the text to be processed.\n",
    "#### Lemmatizing:\n",
    "* The goal of lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. By default, an attempt will be made to find the closest noun of a word.\n",
    "#### Synsets:\n",
    "* WordNet is a lexical database for the English language, and is part of the NLTK corpus. We can use WordNet alongside the NLTK module to find the meaning of words, synonyms, antonyms and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"He loves to play football.\"\n",
    "str2 = \"Football is his favourite sport.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(a):\n",
    "    a=a.lower()\n",
    "    text=re.sub('[^a-z]',' ',a)\n",
    "    text=nltk.word_tokenize(text)\n",
    "    text=[lemma.lemmatize(word) for word in text if word not in stopwords.words('english')]\n",
    "    text=' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=clean_text(str1)\n",
    "sent2=clean_text(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love play football\n",
      "football favourite sport\n"
     ]
    }
   ],
   "source": [
    "print(sent1)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[]\n",
    "for word1 in sent1:\n",
    "    similarity = []\n",
    "    for word2 in sent2:\n",
    "        sims=[]\n",
    "        syns1 = wordnet.synsets(word1)\n",
    "        syns2 = wordnet.synsets(word2)\n",
    "        for sense1, sense2 in product(syns1,syns2):\n",
    "            d = wordnet.wup_similarity(sense1, sense2)\n",
    "            if d != None:\n",
    "                sims.append(d)\n",
    "        if sims != []:        \n",
    "            max_sim = max(sims)\n",
    "            similarity.append(max_sim)\n",
    "    if similarity != []:\n",
    "        max_final = max(similarity)\n",
    "        final.append(max_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9930555555555556"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_index = np.mean(final)\n",
    "similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar\n"
     ]
    }
   ],
   "source": [
    "if similarity_index>0.80:\n",
    "    print(\"Similar\")\n",
    "elif similarity_index>=0.60:\n",
    "    print(\"Somewhat Similar\")\n",
    "else:\n",
    "    print(\"Not Similar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
